name: trunk1

on:
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: 29 8 * * *  # about 1:29am PDT

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}
  cancel-in-progress: true

jobs:
  linux-bionic-cuda12_1-py3_10-gcc9-build:
    name: linux-bionic-cuda12.1-py3.10-gcc9
    uses: ./.github/workflows/_linux-build.yml
    with:
      build-environment: linux-bionic-cuda12.1-py3.10-gcc9
      docker-image-name: pytorch-linux-bionic-cuda12.1-cudnn8-py3-gcc9
      test-matrix: |
        { include: [
          { config: "default", shard: 1, num_shards: 5, runner: "linux.4xlarge.nvidia.gpu" },
          { config: "default", shard: 2, num_shards: 5, runner: "linux.4xlarge.nvidia.gpu" },
          { config: "default", shard: 3, num_shards: 5, runner: "linux.4xlarge.nvidia.gpu" },
          { config: "default", shard: 4, num_shards: 5, runner: "linux.4xlarge.nvidia.gpu" },
          { config: "default", shard: 5, num_shards: 5, runner: "linux.4xlarge.nvidia.gpu" },
          { config: "deploy", shard: 1, num_shards: 1, runner: "linux.4xlarge.nvidia.gpu" },
        ]}

  linux-bionic-cuda12_1-py3_10-gcc9-sm86-build:
    name: linux-bionic-cuda12.1-py3.10-gcc9-sm86
    uses: ./.github/workflows/_linux-build.yml
    with:
      build-environment: linux-bionic-cuda12.1-py3.10-gcc9-sm86
      docker-image-name: pytorch-linux-bionic-cuda12.1-cudnn8-py3-gcc9
      cuda-arch-list: 8.6
      test-matrix: |
        { include: [
          { config: "default", shard: 1, num_shards: 5, runner: "linux.g5.4xlarge.nvidia.gpu" },
          { config: "default", shard: 2, num_shards: 5, runner: "linux.g5.4xlarge.nvidia.gpu" },
          { config: "default", shard: 3, num_shards: 5, runner: "linux.g5.4xlarge.nvidia.gpu" },
          { config: "default", shard: 4, num_shards: 5, runner: "linux.g5.4xlarge.nvidia.gpu" },
          { config: "default", shard: 5, num_shards: 5, runner: "linux.g5.4xlarge.nvidia.gpu" },
        ]}
